{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/DELL/.cache/huggingface/datasets/parquet/plain_text-57edf78d6033ac9a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "320fc2997e5e44fe9b615cfc3168caa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import transformers\n",
    "from datasets import load_dataset\n",
    "# from transformers import AutoModelForQuestionAnswering, BertModel, BertConfig, BertTokenizer, pipeline, AutoTokenizer\n",
    "from transformers import AutoModelForQuestionAnswering, BertConfig, BertTokenizer, pipeline, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"squad\")\n",
    "train = dataset['train']\n",
    "validation = dataset['validation']\n",
    "\n",
    "model_checkpoint = \"atharvamundada99/bert-large-question-answering-finetuned-legal\"\n",
    "pretrained_model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QA(model, tokenizer, question, context):\n",
    "    # Process the inputs\n",
    "    inputs = tokenizer(question, context, return_tensors='pt')\n",
    "\n",
    "    # Pass the inputs through the model and get the start and end scores\n",
    "    start_scores, end_scores = model(**inputs)\n",
    "\n",
    "    # Get the start and end positions\n",
    "    start_position = torch.argmax(start_scores)\n",
    "    end_position = torch.argmax(end_scores)\n",
    "\n",
    "    # Get the answer\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][start_position:end_position+1]))\n",
    "\n",
    "    return answer\n",
    "\n",
    "def QAs(model, tokenizer, questions, contexts):\n",
    "    answers = []\n",
    "    for question, context in zip(questions, contexts):\n",
    "        answer = QA(model, tokenizer, question, context)\n",
    "        answers.append(answer)\n",
    "    return answers\n",
    "\n",
    "def Evaluation(model, tokenizer, validation):\n",
    "    correct = 0\n",
    "    EM = 0\n",
    "    total = 0\n",
    "    errors = []\n",
    "    for record in tqdm(validation):\n",
    "        try:\n",
    "            total += 1\n",
    "            if (total % 500 == 0):\n",
    "                print(f\"\\nAccuracy: {100*correct/total}\")\n",
    "                print(f\"Correct: {correct}, out of {total}\")\n",
    "                print(f\"EM: {100*EM/total}\")\n",
    "                print(f\"EM Correct: {EM}, out of {total}\\n\")\n",
    "\n",
    "            predicted_answer = QA(model, tokenizer, record['question'], record['context'])\n",
    "            if predicted_answer.lower() in record['answers']['text'][0].lower() or record['answers']['text'][0].lower() in predicted_answer.lower():\n",
    "                correct += 1\n",
    "            if predicted_answer.lower() == record['answers']['text'][0].lower():\n",
    "                EM += 1\n",
    "        except Exception as e:\n",
    "            errors.append(total)\n",
    "            print(f\"Error at {total}: {e} \")\n",
    "            continue\n",
    "    return correct, EM, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moved\n",
    "# class BertConfig:\n",
    "#     def __init__(self, vocab_size=30522, hidden_size=1024, num_hidden_layers=24, intermediate_size=4096, num_attention_heads=16, attention_probs_dropout_prob=0.1, max_position_embeddings=512, type_vocab_size=2, **kwargs):\n",
    "#         self.vocab_size = vocab_size\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_hidden_layers = num_hidden_layers\n",
    "#         self.intermediate_size = intermediate_size\n",
    "#         self.num_attention_heads = num_attention_heads\n",
    "#         self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "#         self.max_position_embeddings = max_position_embeddings\n",
    "#         self.type_vocab_size = type_vocab_size\n",
    "#         for key, value in kwargs.items():\n",
    "#             setattr(self, key, value)\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_dict(cls, json_object):\n",
    "#         return cls(**json_object)\n",
    "\n",
    "#     def to_dict(self):\n",
    "#         return self.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbeddings(nn.Module):\n",
    "    def __init__(self, vocab_size=30522, hidden_size=1024, pad_token_id=0, max_position_embeddings=512, type_vocab_size=2):\n",
    "        super(BertEmbeddings, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, hidden_size, padding_idx=pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(type_vocab_size, hidden_size)\n",
    "\n",
    "        # Make position_ids a nn.Parameter\n",
    "        self.position_ids = nn.Parameter(torch.arange(max_position_embeddings).unsqueeze(0), requires_grad=False)\n",
    "\n",
    "        # LayerNorm and dropout Module\n",
    "        self.LayerNorm = nn.LayerNorm(hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, input_ids=None, token_type_ids=None, position_ids=None):\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, :input_ids.size(1)]  # use pre-computed position_ids\n",
    "\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        word_embeddings = self.word_embeddings(input_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        if position_embeddings.size(1) < word_embeddings.size(1):       # to handle size mismatch by padding\n",
    "            padding = torch.zeros((position_embeddings.size(0), word_embeddings.size(1) - position_embeddings.size(1), position_embeddings.size(2)), device=position_embeddings.device)\n",
    "            position_embeddings = torch.cat([position_embeddings, padding], dim=1)\n",
    "\n",
    "        embeddings = word_embeddings + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads, dropout_prob):\n",
    "        super(BertSelfAttention, self).__init__()\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.attention_head_size = int(hidden_size / num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        if attention_mask is not None:\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        return context_layer\n",
    "\n",
    "class BertSelfOutput(nn.Module):\n",
    "    def __init__(self, hidden_size=1024, dropout_prob=0.1):\n",
    "        super(BertSelfOutput, self).__init__()\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        # Implement the forward pass\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, hidden_size=1024, num_attention_heads=16, attention_probs_dropout_prob=0.1):\n",
    "        super(BertAttention, self).__init__()\n",
    "\n",
    "        self.self = BertSelfAttention(hidden_size, num_attention_heads, attention_probs_dropout_prob)\n",
    "        self.output = BertSelfOutput(hidden_size, attention_probs_dropout_prob)\n",
    "\n",
    "    def forward(self, input_tensor, attention_mask):\n",
    "        # Implement the forward pass\n",
    "        self_output = self.self(input_tensor, attention_mask)\n",
    "        if isinstance(self_output, tuple):\n",
    "            self_output = self_output[0]\n",
    "        attention_output = self.output(self_output, input_tensor)\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELUActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.gelu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self, hidden_size=1024, intermediate_size=4096):\n",
    "        super(BertIntermediate, self).__init__()\n",
    "        self.dense = nn.Linear(hidden_size, intermediate_size)\n",
    "        self.intermediate_act_fn = GELUActivation()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = F.gelu(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertOutput(nn.Module):\n",
    "    def __init__(self, intermediate_size=4096, hidden_size=1024, dropout_prob=0.1):\n",
    "        super(BertOutput, self).__init__()\n",
    "        self.dense = nn.Linear(intermediate_size, hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        # Implement the forward pass\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, hidden_size=1024, intermediate_size=4096, num_attention_heads=16, attention_probs_dropout_prob=0.1):\n",
    "        super(BertLayer, self).__init__()\n",
    "        self.attention = BertAttention(hidden_size, num_attention_heads, attention_probs_dropout_prob)\n",
    "        self.intermediate = BertIntermediate(hidden_size, intermediate_size)\n",
    "        self.output = BertOutput(intermediate_size, hidden_size, attention_probs_dropout_prob)\n",
    "\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        # Implement the forward pass\n",
    "        attention_output = self.attention(hidden_states, attention_mask)\n",
    "        if isinstance(attention_output, tuple):\n",
    "                attention_output = attention_output[0]\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, num_hidden_layers=24, hidden_size=1024, intermediate_size=4096, num_attention_heads=16, attention_probs_dropout_prob=0.1):\n",
    "        super(BertEncoder, self).__init__()\n",
    "        self.layer = nn.ModuleList([BertLayer(hidden_size, intermediate_size, num_attention_heads, attention_probs_dropout_prob) for _ in range(num_hidden_layers)])\n",
    "        \n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        # Implement the forward pass\n",
    "        for layer in self.layer:\n",
    "            # check type of hidden_states\n",
    "            if isinstance(hidden_states, tuple):\n",
    "                hidden_states = hidden_states[0]\n",
    "            hidden_states = layer(hidden_states, attention_mask)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No longer needed\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class BertPooler(nn.Module):\n",
    "#     def __init__(self, hidden_size=1024):\n",
    "#         super(BertPooler, self).__init__()\n",
    "#         self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.activation = nn.Tanh()\n",
    "\n",
    "#     def forward(self, hidden_states):\n",
    "#         # We \"pool\" the model by simply taking the hidden state corresponding to the first token.\n",
    "#         first_token_tensor = hidden_states[:, 0]\n",
    "#         pooled_output = self.dense(first_token_tensor)\n",
    "#         pooled_output = self.activation(pooled_output)\n",
    "#         return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(nn.Module):\n",
    "    def __init__(self, vocab_size=30522, hidden_size=1024, num_hidden_layers=24, intermediate_size=4096, num_attention_heads=16, attention_probs_dropout_prob=0.1, pad_token_id = 0, max_position_embeddings=512, type_vocab_size=2):\n",
    "        super(BertModel, self).__init__()\n",
    "        self.embeddings = BertEmbeddings(vocab_size, hidden_size, pad_token_id, max_position_embeddings, type_vocab_size)\n",
    "        self.encoder = BertEncoder(num_hidden_layers, hidden_size, intermediate_size, num_attention_heads, attention_probs_dropout_prob)\n",
    "        \n",
    "\n",
    "        # self.pooler = BertPooler(hidden_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        # Implement the forward pass\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
    "        encoder_output = self.encoder(embedding_output, extended_attention_mask)\n",
    "        # pooled_output = self.pooler(encoder_output)\n",
    "\n",
    "        return encoder_output\n",
    "        # return pooled_output  # or return pooled_output lw hnst3ml el pooler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBertForQuestionAnswering(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(CustomBertForQuestionAnswering, self).__init__()\n",
    "        self.config = config\n",
    "        self.bert = BertModel(vocab_size=config.vocab_size, hidden_size=config.hidden_size, num_hidden_layers=config.num_hidden_layers, intermediate_size=config.intermediate_size, num_attention_heads=config.num_attention_heads, attention_probs_dropout_prob=config.attention_probs_dropout_prob, pad_token_id=config.pad_token_id ,max_position_embeddings=config.max_position_embeddings, type_vocab_size=config.type_vocab_size)\n",
    "        \n",
    "        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        sequence_output = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "        if isinstance(sequence_output, tuple):\n",
    "            sequence_output = sequence_output[0]\n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "        return start_logits, end_logits\n",
    "\n",
    "# Instantiate the model with the provided configuration\n",
    "config = BertConfig.from_dict({\n",
    "    \"_name_or_path\": \"ourModel\",\n",
    "    \"architectures\": [\n",
    "        \"BertForQuestionAnswering\"\n",
    "    ],\n",
    "    \"attention_probs_dropout_prob\": 0.1,\n",
    "    \"gradient_checkpointing\": False,\n",
    "    \"hidden_act\": \"gelu\",\n",
    "    \"hidden_dropout_prob\": 0.1,\n",
    "    \"hidden_size\": 1024,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"intermediate_size\": 4096,\n",
    "    \"layer_norm_eps\": 1e-12,\n",
    "    \"max_position_embeddings\": 512,\n",
    "    \"model_type\": \"bert\",\n",
    "    \"num_attention_heads\": 16,\n",
    "    \"num_hidden_layers\": 24,\n",
    "    \"pad_token_id\": 0,\n",
    "    \"position_embedding_type\": \"absolute\",\n",
    "    \"transformers_version\": \"4.17.0\",\n",
    "    \"type_vocab_size\": 2,\n",
    "    \"use_cache\": True,\n",
    "    \"vocab_size\": 30522\n",
    "})\n",
    "\n",
    "model = CustomBertForQuestionAnswering(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392\n",
      "392\n",
      "392\n",
      "392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get state dictionary of pre-trained model\n",
    "pretrained_dict = pretrained_model.state_dict()\n",
    "\n",
    "# Get state dictionary of custom model\n",
    "model_dict = model.state_dict()\n",
    "\n",
    "print(len(pretrained_dict))\n",
    "print(len(model_dict))\n",
    "\n",
    "\n",
    "# Check the keys that are not in the model_dict\n",
    "for k, v in pretrained_dict.items():\n",
    "    if k not in model_dict:\n",
    "        print(k, \":\", v.shape)\n",
    "\n",
    "# Filter out unnecessary keys\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "    \n",
    "\n",
    "print(len(pretrained_dict))\n",
    "print(len(model_dict))\n",
    "\n",
    "# Overwrite entries in the existing state dict\n",
    "model_dict.update(pretrained_dict)\n",
    "\n",
    "# Load the new state dict\n",
    "model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(model.training)\n",
    "model.eval()\n",
    "print(model.training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(pretrained_model.training)\n",
    "pretrained_model.eval()\n",
    "print(pretrained_model.training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'CustomBertForQuestionAnswering' is not supported for question-answering. Supported models are ['YosoForQuestionAnswering', 'NystromformerForQuestionAnswering', 'QDQBertForQuestionAnswering', 'FNetForQuestionAnswering', 'GPTJForQuestionAnswering', 'LayoutLMv2ForQuestionAnswering', 'RemBertForQuestionAnswering', 'CanineForQuestionAnswering', 'RoFormerForQuestionAnswering', 'BigBirdPegasusForQuestionAnswering', 'BigBirdForQuestionAnswering', 'ConvBertForQuestionAnswering', 'LEDForQuestionAnswering', 'DistilBertForQuestionAnswering', 'AlbertForQuestionAnswering', 'CamembertForQuestionAnswering', 'BartForQuestionAnswering', 'MBartForQuestionAnswering', 'LongformerForQuestionAnswering', 'XLMRobertaXLForQuestionAnswering', 'XLMRobertaForQuestionAnswering', 'RobertaForQuestionAnswering', 'SqueezeBertForQuestionAnswering', 'BertForQuestionAnswering', 'XLNetForQuestionAnsweringSimple', 'FlaubertForQuestionAnsweringSimple', 'MegatronBertForQuestionAnswering', 'MobileBertForQuestionAnswering', 'XLMForQuestionAnsweringSimple', 'ElectraForQuestionAnswering', 'ReformerForQuestionAnswering', 'FunnelForQuestionAnswering', 'LxmertForQuestionAnswering', 'MPNetForQuestionAnswering', 'DebertaForQuestionAnswering', 'DebertaV2ForQuestionAnswering', 'IBertForQuestionAnswering', 'SplinterForQuestionAnswering', 'Data2VecTextForQuestionAnswering'].\n"
     ]
    }
   ],
   "source": [
    "question_answerer = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'CustomModel.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 114/10570 [00:38<32:17,  5.40it/s]  c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "  4%|▍         | 439/10570 [03:37<1:23:17,  2.03it/s]c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "  9%|▉         | 999/10570 [09:01<1:17:51,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct:\t\t 942, out of 1000: 94.2%\n",
      "EM:\t\t\t 762, out of 1000: 76.2%\n",
      "BLEU:\t\t\t 878, out of 1000: 87.8%\n",
      "BLEU Score:\t\t 872.7723680467068, out of 1000: 87.27723680467068%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 1005/10570 [09:05<1:26:34,  1.84it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLEU:\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBLEU\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, out of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39mBLEU\u001b[38;5;241m/\u001b[39mtotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLEU Score:\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbblleeuu\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, out of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39mbblleeuu\u001b[38;5;241m/\u001b[39mtotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m result \u001b[38;5;241m=\u001b[39m question_answerer(question\u001b[38;5;241m=\u001b[39mrecord[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m], context\u001b[38;5;241m=\u001b[39mrecord[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m], truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# result = QA(model, tokenizer,record['question'], record['context'])\u001b[39;00m\n\u001b[0;32m     17\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit()), \u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\question_answering.py:251\u001b[0m, in \u001b[0;36mQuestionAnsweringPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    249\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args_parser(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(examples) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(examples[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(examples, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1027\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1027\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1049\u001b[0m, in \u001b[0;36mChunkPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1047\u001b[0m all_outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params):\n\u001b[1;32m-> 1049\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1050\u001b[0m     all_outputs\u001b[38;5;241m.\u001b[39mappend(model_outputs)\n\u001b[0;32m   1051\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(all_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:944\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m    942\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m    943\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 944\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m    945\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    946\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\question_answering.py:371\u001b[0m, in \u001b[0;36mQuestionAnsweringPipeline._forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    369\u001b[0m example \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    370\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m {k: inputs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mmodel_input_names}\n\u001b[1;32m--> 371\u001b[0m start, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs)[:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m: start, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m: end, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample\u001b[39m\u001b[38;5;124m\"\u001b[39m: example, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs}\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 10\u001b[0m, in \u001b[0;36mCustomBertForQuestionAnswering.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, token_type_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 10\u001b[0m     sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert(input_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sequence_output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m     13\u001b[0m         sequence_output \u001b[38;5;241m=\u001b[39m sequence_output[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 17\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids)\u001b[0m\n\u001b[0;32m     14\u001b[0m extended_attention_mask \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m extended_attention_mask) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10000.0\u001b[39m\n\u001b[0;32m     16\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(input_ids, token_type_ids)\n\u001b[1;32m---> 17\u001b[0m encoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(embedding_output, extended_attention_mask)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# pooled_output = self.pooler(encoder_output)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoder_output\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 30\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(hidden_states, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m     29\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m hidden_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 30\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer(hidden_states, attention_mask)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 11\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states, attention_mask):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Implement the forward pass\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(hidden_states, attention_mask)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(attention_output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m     13\u001b[0m             attention_output \u001b[38;5;241m=\u001b[39m attention_output[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 13\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, input_tensor, attention_mask)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(self_output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m     12\u001b[0m     self_output \u001b[38;5;241m=\u001b[39m self_output[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 13\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_output, input_tensor)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attention_output\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 52\u001b[0m, in \u001b[0;36mBertSelfOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states, input_tensor):\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m# Implement the forward pass\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m     53\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m     54\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "EM = 0\n",
    "BLEU = 0\n",
    "bblleeuu = 0\n",
    "errors = []\n",
    "for record in tqdm(validation):\n",
    "        # try:\n",
    "                total += 1\n",
    "                if (total % 1000 == 0):\n",
    "                        print(f\"Correct:\\t\\t {correct}, out of {total}: {100*correct/total}%\")\n",
    "                        print(f\"EM:\\t\\t\\t {EM}, out of {total}: {100*EM/total}%\")\n",
    "                        print(f\"BLEU:\\t\\t\\t {BLEU}, out of {total}: {100*BLEU/total}%\")\n",
    "                        print(f\"BLEU Score:\\t\\t {bblleeuu}, out of {total}: {100*bblleeuu/total}%\")\n",
    "                result = question_answerer(question=record['question'], context=record['context'], truncation=True, padding=True, return_tensors='pt')\n",
    "                # result = QA(model, tokenizer,record['question'], record['context'])\n",
    "                n = min(len(result['answer'].split()), 4)\n",
    "                # n = min(len(result.split()), 4)\n",
    "                if n == 0:\n",
    "                        BLEUscore = 0\n",
    "                else:\n",
    "                        weights = [1.0/n]*n\n",
    "                        BLEUscore = nltk.translate.bleu_score.sentence_bleu([record['answers']['text'][0].lower()], result['answer'].lower(), weights=weights)\n",
    "                        # BLEUscore = nltk.translate.bleu_score.sentence_bleu([record['answers']['text'][0].lower()], result.lower(), weights=weights)\n",
    "                if result['answer'] != '' and (result['answer'].lower() in record['answers']['text'][0].lower() or record['answers']['text'][0].lower() in result['answer'].lower()):\n",
    "                # if result != '' and (result.lower() in record['answers']['text'][0].lower() or record['answers']['text'][0].lower() in result.lower()):\n",
    "                        correct += 1\n",
    "                if record['answers']['text'][0].lower() == result['answer'].lower():\n",
    "                # if record['answers']['text'][0].lower() == result.lower():\n",
    "                        EM += 1\n",
    "                if BLEUscore > 0.5:\n",
    "                        BLEU += 1\n",
    "                bblleeuu += BLEUscore\n",
    "                 \n",
    "        # except Exception as e:\n",
    "        #         errors.append(total)\n",
    "        #         print(f\"Error at {total}: {e}\")\n",
    "        #         continue\n",
    "print(f\"Correct: {correct}, out of {total}: {100*correct/total}%\")\n",
    "print(f\"EM: {EM}, out of {total}: {100*EM/total}%\")\n",
    "print(f\"BLEU: {BLEU}, out of {total}: {100*BLEU/total}%\")\n",
    "print(f\"BLEU Score: {bblleeuu}, out of {total}: {100*bblleeuu/total}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dh bl pipeline el ndeefa (el model kan fl eval mode), el tany aw7ash men dyh\n",
    "# 1%|          | 114/10570 [00:32<27:50,  6.26it/s] c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
    "# The hypothesis contains 0 counts of 3-gram overlaps.\n",
    "# Therefore the BLEU score evaluates to 0, independently of\n",
    "# how many N-gram overlaps of lower order it contains.\n",
    "# Consider using lower n-gram order or use SmoothingFunction()\n",
    "#   warnings.warn(_msg)\n",
    "# c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
    "# The hypothesis contains 0 counts of 4-gram overlaps.\n",
    "# Therefore the BLEU score evaluates to 0, independently of\n",
    "# how many N-gram overlaps of lower order it contains.\n",
    "# Consider using lower n-gram order or use SmoothingFunction()\n",
    "#   warnings.warn(_msg)\n",
    "#   4%|▍         | 439/10570 [04:40<2:04:55,  1.35it/s]c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
    "# The hypothesis contains 0 counts of 2-gram overlaps.\n",
    "# Therefore the BLEU score evaluates to 0, independently of\n",
    "# how many N-gram overlaps of lower order it contains.\n",
    "# Consider using lower n-gram order or use SmoothingFunction()\n",
    "#   warnings.warn(_msg)\n",
    "#   9%|▉         | 999/10570 [08:31<56:16,  2.83it/s]  \n",
    "# Correct:\t\t\t 942, out of 1000: 94.2%\n",
    "# EM:\t\t\t\t 762, out of 1000: 76.2%\n",
    "# BLEU:\t\t\t\t 878, out of 1000: 87.8%\n",
    "# BLEU Score:\t\t 872.7723680467068, out of 1000: 87.27723680467068\n",
    "#  19%|█▉        | 1999/10570 [13:49<51:12,  2.79it/s]  \n",
    "# Correct:\t\t\t 1869, out of 2000: 93.45%\n",
    "# EM:\t\t\t\t 1432, out of 2000: 71.6%\n",
    "# BLEU:\t\t\t\t 1703, out of 2000: 85.15%\n",
    "# BLEU Score:\t\t 1697.4421047529827, out of 2000: 84.87210523764912\n",
    "#  28%|██▊       | 2999/10570 [19:44<43:38,  2.89it/s]  \n",
    "# Correct:\t\t\t 2799, out of 3000: 93.3%\n",
    "# EM:\t\t\t\t 2083, out of 3000: 69.43333333333334%\n",
    "# BLEU:\t\t\t\t 2507, out of 3000: 83.56666666666666%\n",
    "# BLEU Score:\t\t 2508.141037469119, out of 3000: 83.60470124897064\n",
    "#  38%|███▊      | 3999/10570 [26:03<46:35,  2.35it/s]  \n",
    "# Correct:\t\t\t 3728, out of 4000: 93.2%\n",
    "# EM:\t\t\t\t 2750, out of 4000: 68.75%\n",
    "# BLEU:\t\t\t\t 3308, out of 4000: 82.7%\n",
    "# BLEU Score:\t\t 3317.115421181209, out of 4000: 82.92788552953022\n",
    "#  47%|████▋     | 4999/10570 [41:24<2:08:46,  1.39s/it]\n",
    "# Correct:\t\t\t 4651, out of 5000: 93.02%\n",
    "# EM:\t\t\t\t 3317, out of 5000: 66.34%\n",
    "# BLEU:\t\t\t\t 4056, out of 5000: 81.12%\n",
    "# BLEU Score:\t\t 4067.8638478020785, out of 5000: 81.35727695604156\n",
    "#  57%|█████▋    | 5999/10570 [53:28<53:14,  1.43it/s]  \n",
    "# Correct:\t\t\t 5601, out of 6000: 93.35%\n",
    "# EM:\t\t\t\t 4042, out of 6000: 67.36666666666666%\n",
    "# BLEU:\t\t\t\t 4891, out of 6000: 81.51666666666667%\n",
    "# BLEU Score:\t\t 4909.089732703673, out of 6000: 81.81816221172788\n",
    "#  66%|██████▌   | 6999/10570 [1:07:14<27:44,  2.15it/s]  \n",
    "# Correct:\t\t\t 6494, out of 7000: 92.77142857142857%\n",
    "# EM:\t\t\t\t 4693, out of 7000: 67.04285714285714%\n",
    "# BLEU:\t\t\t\t 5667, out of 7000: 80.95714285714286%\n",
    "# BLEU Score:\t\t 5691.8794216420865, out of 7000: 81.31256316631551\n",
    "#  76%|███████▌  | 7999/10570 [1:16:37<27:38,  1.55it/s]  \n",
    "# Correct:\t\t\t 7429, out of 8000: 92.8625%\n",
    "# EM:\t\t\t\t 5390, out of 8000: 67.375%\n",
    "# BLEU:\t\t\t\t 6491, out of 8000: 81.1375%\n",
    "# BLEU Score:\t\t 6516.97302106118, out of 8000: 81.46216276326474\n",
    "#  85%|████████▌ | 8999/10570 [1:28:00<11:31,  2.27it/s]  \n",
    "# Correct:\t\t\t 8362, out of 9000: 92.91111111111111%\n",
    "# EM:\t\t\t\t 5986, out of 9000: 66.5111111111111%\n",
    "# BLEU:\t\t\t\t 7270, out of 9000: 80.77777777777777%\n",
    "# BLEU Score:\t\t 7292.617317963844, out of 9000: 81.02908131070937\n",
    "#  95%|█████████▍| 9999/10570 [1:43:21<07:44,  1.23it/s]\n",
    "# Correct:\t\t\t 9279, out of 10000: 92.79%\n",
    "# EM:\t\t\t\t 6578, out of 10000: 65.78%\n",
    "# BLEU:\t\t\t\t 8042, out of 10000: 80.42%\n",
    "# BLEU Score:\t\t 8068.880321520323, out of 10000: 80.68880321520324\n",
    "# 100%|██████████| 10570/10570 [1:52:25<00:00,  1.57it/s]\n",
    "# Correct: 9789, out of 10570: 92.61116367076632%\n",
    "# EM: 6901, out of 10570: 65.28855250709556%\n",
    "# BLEU: 8477, out of 10570: 80.19867549668874%\n",
    "# BLEU Score: 8501.452133048304, out of 10570: 80.43001071947307\n",
    "\n",
    "\n",
    "# ===================================================================================================================\n",
    "# hena kan fl training mode (aw7ash mn el eval)\n",
    "#   9%|▉         | 999/10570 [11:38<1:59:37,  1.33it/s] \n",
    "# Correct:\t\t 932, out of 1000: 93.2%\n",
    "# EM:\t\t\t 745, out of 1000: 74.5%\n",
    "# BLEU:\t\t\t 864, out of 1000: 86.4%\n",
    "# BLEU Score:\t\t 861.1770426881361, out of 1000: 86.11770426881361%\n",
    "#  19%|█▉        | 1999/10570 [20:07<1:51:32,  1.28it/s]\n",
    "# Correct:\t\t 1855, out of 2000: 92.75%\n",
    "# EM:\t\t\t 1411, out of 2000: 70.55%\n",
    "# BLEU:\t\t\t 1674, out of 2000: 83.7%\n",
    "# BLEU Score:\t\t 1676.7278192732056, out of 2000: 83.83639096366028%\n",
    "#  28%|██▊       | 2999/10570 [32:43<1:33:01,  1.36it/s]\n",
    "# Correct:\t\t 2780, out of 3000: 92.66666666666667%\n",
    "# EM:\t\t\t 2039, out of 3000: 67.96666666666667%\n",
    "# BLEU:\t\t\t 2465, out of 3000: 82.16666666666667%\n",
    "# BLEU Score:\t\t 2472.351418400786, out of 3000: 82.41171394669286%\n",
    "#  38%|███▊      | 3999/10570 [46:19<1:40:16,  1.09it/s]\n",
    "# Correct:\t\t 3702, out of 4000: 92.55%\n",
    "# EM:\t\t\t 2694, out of 4000: 67.35%\n",
    "# BLEU:\t\t\t 3255, out of 4000: 81.375%\n",
    "# BLEU Score:\t\t 3270.4277132334437, out of 4000: 81.76069283083609%\n",
    "#  47%|████▋     | 4999/10570 [1:05:18<2:03:05,  1.33s/it]\n",
    "# Correct:\t\t 4615, out of 5000: 92.3%\n",
    "# EM:\t\t\t 3260, out of 5000: 65.2%\n",
    "# BLEU:\t\t\t 3998, out of 5000: 79.96%\n",
    "# BLEU Score:\t\t 4018.995368299551, out of 5000: 80.37990736599103%\n",
    "#  57%|█████▋    | 5999/10570 [1:16:29<46:29,  1.64it/s]  \n",
    "# Correct:\t\t 5566, out of 6000: 92.76666666666667%\n",
    "# EM:\t\t\t 3975, out of 6000: 66.25%\n",
    "# BLEU:\t\t\t 4824, out of 6000: 80.4%\n",
    "# BLEU Score:\t\t 4855.943115278409, out of 6000: 80.93238525464015%\n",
    "#  66%|██████▌   | 6999/10570 [1:25:45<22:18,  2.67it/s]  \n",
    "# Correct:\t\t 6462, out of 7000: 92.31428571428572%\n",
    "# EM:\t\t\t 4616, out of 7000: 65.94285714285714%\n",
    "# BLEU:\t\t\t 5590, out of 7000: 79.85714285714286%\n",
    "# BLEU Score:\t\t 5629.781707379695, out of 7000: 80.42545296256706%\n",
    "#  76%|███████▌  | 7999/10570 [1:33:31<22:30,  1.90it/s]  \n",
    "# Correct:\t\t 7388, out of 8000: 92.35%\n",
    "# EM:\t\t\t 5300, out of 8000: 66.25%\n",
    "# BLEU:\t\t\t 6398, out of 8000: 79.975%\n",
    "# BLEU Score:\t\t 6441.607009922551, out of 8000: 80.52008762403189%\n",
    "#  85%|████████▌ | 8999/10570 [1:46:01<20:18,  1.29it/s]  \n",
    "# Correct:\t\t 8313, out of 9000: 92.36666666666666%\n",
    "# EM:\t\t\t 5881, out of 9000: 65.34444444444445%\n",
    "# BLEU:\t\t\t 7162, out of 9000: 79.57777777777778%\n",
    "# BLEU Score:\t\t 7205.7352295038, out of 9000: 80.06372477226445%\n",
    "#  95%|█████████▍| 9999/10570 [1:54:27<07:04,  1.34it/s]\n",
    "# Correct:\t\t 9231, out of 10000: 92.31%\n",
    "# EM:\t\t\t 6479, out of 10000: 64.79%\n",
    "# BLEU:\t\t\t 7938, out of 10000: 79.38%\n",
    "# BLEU Score:\t\t 7989.309027525096, out of 10000: 79.89309027525096%\n",
    "# 100%|██████████| 10570/10570 [2:02:19<00:00,  1.44it/s]\n",
    "# Correct: 9737, out of 10570: 92.11920529801324%\n",
    "# EM: 6801, out of 10570: 64.34247871333964%\n",
    "# BLEU: 8369, out of 10570: 79.17691579943235%\n",
    "# BLEU Score: 8420.278207712268, out of 10570: 79.66204548450585%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
