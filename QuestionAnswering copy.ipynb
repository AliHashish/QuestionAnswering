{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/DELL/.cache/huggingface/datasets/parquet/plain_text-57edf78d6033ac9a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28001e0c47a4407eb14801c4e84be4da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import transformers\n",
    "from datasets import load_dataset\n",
    "# from transformers import AutoModelForQuestionAnswering, BertModel, BertConfig, BertTokenizer, pipeline, AutoTokenizer\n",
    "from transformers import AutoModelForQuestionAnswering, BertConfig, BertTokenizer, pipeline, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import nltk\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"squad\")\n",
    "train = dataset['train']\n",
    "validation = dataset['validation']\n",
    "\n",
    "# question_answerer = pipeline(\"question-answering\", model='AliHashish/distilbert-base-uncased-finetuned-squad-EZcufe')\n",
    "model_checkpoint = \"atharvamundada99/bert-large-question-answering-finetuned-legal\"\n",
    "pretrained_model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QA(model, tokenizer, question, context):\n",
    "    # Process the inputs\n",
    "    inputs = tokenizer(question, context, return_tensors='pt')\n",
    "\n",
    "    # Pass the inputs through the model and get the start and end scores\n",
    "    start_scores, end_scores = model(**inputs)\n",
    "\n",
    "    # Get the start and end positions\n",
    "    start_position = torch.argmax(start_scores)\n",
    "    end_position = torch.argmax(end_scores)\n",
    "\n",
    "    # Get the answer\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][start_position:end_position+1]))\n",
    "\n",
    "    return answer\n",
    "\n",
    "def QAs(model, tokenizer, questions, contexts):\n",
    "    answers = []\n",
    "    for question, context in zip(questions, contexts):\n",
    "        answer = QA(model, tokenizer, question, context)\n",
    "        answers.append(answer)\n",
    "    return answers\n",
    "\n",
    "def Evaluation(model, tokenizer, validation):\n",
    "    correct = 0\n",
    "    EM = 0\n",
    "    total = 0\n",
    "    errors = []\n",
    "    for record in tqdm(validation):\n",
    "        try:\n",
    "            total += 1\n",
    "            if (total % 500 == 0):\n",
    "                print(f\"\\nAccuracy: {100*correct/total}\")\n",
    "                print(f\"Correct: {correct}, out of {total}\")\n",
    "                print(f\"EM: {100*EM/total}\")\n",
    "                print(f\"EM Correct: {EM}, out of {total}\\n\")\n",
    "\n",
    "            predicted_answer = QA(model, tokenizer, record['question'], record['context'])\n",
    "            if predicted_answer.lower() in record['answers']['text'][0].lower() or record['answers']['text'][0].lower() in predicted_answer.lower():\n",
    "                correct += 1\n",
    "            if predicted_answer.lower() == record['answers']['text'][0].lower():\n",
    "                EM += 1\n",
    "        except Exception as e:\n",
    "            errors.append(total)\n",
    "            print(f\"Error at {total}: {e} \")\n",
    "            continue\n",
    "    return correct, EM, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moved\n",
    "# class BertConfig:\n",
    "#     def __init__(self, vocab_size=30522, hidden_size=1024, num_hidden_layers=24, intermediate_size=4096, num_attention_heads=16, attention_probs_dropout_prob=0.1, max_position_embeddings=512, type_vocab_size=2, **kwargs):\n",
    "#         self.vocab_size = vocab_size\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_hidden_layers = num_hidden_layers\n",
    "#         self.intermediate_size = intermediate_size\n",
    "#         self.num_attention_heads = num_attention_heads\n",
    "#         self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "#         self.max_position_embeddings = max_position_embeddings\n",
    "#         self.type_vocab_size = type_vocab_size\n",
    "#         for key, value in kwargs.items():\n",
    "#             setattr(self, key, value)\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_dict(cls, json_object):\n",
    "#         return cls(**json_object)\n",
    "\n",
    "#     def to_dict(self):\n",
    "#         return self.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BertEmbeddings(nn.Module):\n",
    "    def __init__(self, vocab_size=30522, hidden_size=1024, pad_token_id=0, max_position_embeddings=512, type_vocab_size=2):\n",
    "        super(BertEmbeddings, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, hidden_size, padding_idx=pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(type_vocab_size, hidden_size)\n",
    "\n",
    "        # Make position_ids a nn.Parameter\n",
    "        self.position_ids = nn.Parameter(torch.arange(max_position_embeddings).unsqueeze(0), requires_grad=False)\n",
    "\n",
    "        # LayerNorm and dropout Module\n",
    "        self.LayerNorm = nn.LayerNorm(hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, input_ids=None, token_type_ids=None, position_ids=None):\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, :input_ids.size(1)]  # use pre-computed position_ids\n",
    "\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        word_embeddings = self.word_embeddings(input_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        if position_embeddings.size(1) < word_embeddings.size(1):       # to handle size mismatch by padding\n",
    "            padding = torch.zeros((position_embeddings.size(0), word_embeddings.size(1) - position_embeddings.size(1), position_embeddings.size(2)), device=position_embeddings.device)\n",
    "            position_embeddings = torch.cat([position_embeddings, padding], dim=1)\n",
    "\n",
    "        embeddings = word_embeddings + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_attention_heads, dropout_prob):\n",
    "        super(BertSelfAttention, self).__init__()\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.attention_head_size = int(hidden_size / num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        if attention_mask is not None:\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        return context_layer\n",
    "\n",
    "class BertSelfOutput(nn.Module):\n",
    "    def __init__(self, hidden_size=1024, dropout_prob=0.1):\n",
    "        super(BertSelfOutput, self).__init__()\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        # Implement the forward pass\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, hidden_size=1024, num_attention_heads=16, attention_probs_dropout_prob=0.1):\n",
    "        super(BertAttention, self).__init__()\n",
    "\n",
    "        self.self = BertSelfAttention(hidden_size, num_attention_heads, attention_probs_dropout_prob)\n",
    "        self.output = BertSelfOutput(hidden_size, attention_probs_dropout_prob)\n",
    "\n",
    "    def forward(self, input_tensor, attention_mask):\n",
    "        # Implement the forward pass\n",
    "        self_output = self.self(input_tensor, attention_mask)\n",
    "        if isinstance(self_output, tuple):\n",
    "            self_output = self_output[0]\n",
    "        attention_output = self.output(self_output, input_tensor)\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GELUActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.gelu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self, hidden_size=1024, intermediate_size=4096):\n",
    "        super(BertIntermediate, self).__init__()\n",
    "        self.dense = nn.Linear(hidden_size, intermediate_size)\n",
    "        self.intermediate_act_fn = GELUActivation()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = F.gelu(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BertOutput(nn.Module):\n",
    "    def __init__(self, intermediate_size=4096, hidden_size=1024, dropout_prob=0.1):\n",
    "        super(BertOutput, self).__init__()\n",
    "        self.dense = nn.Linear(intermediate_size, hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        # Implement the forward pass\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, hidden_size=1024, intermediate_size=4096, num_attention_heads=16, attention_probs_dropout_prob=0.1):\n",
    "        super(BertLayer, self).__init__()\n",
    "        self.attention = BertAttention(hidden_size, num_attention_heads, attention_probs_dropout_prob)\n",
    "        self.intermediate = BertIntermediate(hidden_size, intermediate_size)\n",
    "        self.output = BertOutput(intermediate_size, hidden_size, attention_probs_dropout_prob)\n",
    "\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        # Implement the forward pass\n",
    "        attention_output = self.attention(hidden_states, attention_mask)\n",
    "        if isinstance(attention_output, tuple):\n",
    "                attention_output = attention_output[0]\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, num_hidden_layers=24, hidden_size=1024, intermediate_size=4096, num_attention_heads=16, attention_probs_dropout_prob=0.1):\n",
    "        super(BertEncoder, self).__init__()\n",
    "        self.layer = nn.ModuleList([BertLayer(hidden_size, intermediate_size, num_attention_heads, attention_probs_dropout_prob) for _ in range(num_hidden_layers)])\n",
    "        \n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        # Implement the forward pass\n",
    "        for layer in self.layer:\n",
    "            # check type of hidden_states\n",
    "            if isinstance(hidden_states, tuple):\n",
    "                hidden_states = hidden_states[0]\n",
    "            hidden_states = layer(hidden_states, attention_mask)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No longer needed\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class BertPooler(nn.Module):\n",
    "#     def __init__(self, hidden_size=1024):\n",
    "#         super(BertPooler, self).__init__()\n",
    "#         self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.activation = nn.Tanh()\n",
    "\n",
    "#     def forward(self, hidden_states):\n",
    "#         # We \"pool\" the model by simply taking the hidden state corresponding to the first token.\n",
    "#         first_token_tensor = hidden_states[:, 0]\n",
    "#         pooled_output = self.dense(first_token_tensor)\n",
    "#         pooled_output = self.activation(pooled_output)\n",
    "#         return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BertModel(nn.Module):\n",
    "    def __init__(self, vocab_size=30522, hidden_size=1024, num_hidden_layers=24, intermediate_size=4096, num_attention_heads=16, attention_probs_dropout_prob=0.1, pad_token_id = 0, max_position_embeddings=512, type_vocab_size=2):\n",
    "        super(BertModel, self).__init__()\n",
    "        self.embeddings = BertEmbeddings(vocab_size, hidden_size, pad_token_id, max_position_embeddings, type_vocab_size)\n",
    "        self.encoder = BertEncoder(num_hidden_layers, hidden_size, intermediate_size, num_attention_heads, attention_probs_dropout_prob)\n",
    "        \n",
    "\n",
    "        # self.pooler = BertPooler(hidden_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        # Implement the forward pass\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
    "        encoder_output = self.encoder(embedding_output, extended_attention_mask)\n",
    "        # pooled_output = self.pooler(encoder_output)\n",
    "\n",
    "        return encoder_output\n",
    "        # return pooled_output  # or return pooled_output lw hnst3ml el pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CustomBertForQuestionAnswering(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(CustomBertForQuestionAnswering, self).__init__()\n",
    "        self.config = config\n",
    "        self.bert = BertModel(vocab_size=config.vocab_size, hidden_size=config.hidden_size, num_hidden_layers=config.num_hidden_layers, intermediate_size=config.intermediate_size, num_attention_heads=config.num_attention_heads, attention_probs_dropout_prob=config.attention_probs_dropout_prob, pad_token_id=config.pad_token_id ,max_position_embeddings=config.max_position_embeddings, type_vocab_size=config.type_vocab_size)\n",
    "        \n",
    "        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        sequence_output = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "        if isinstance(sequence_output, tuple):\n",
    "            sequence_output = sequence_output[0]\n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "        return start_logits, end_logits\n",
    "\n",
    "# Instantiate the model with the provided configuration\n",
    "config = BertConfig.from_dict({\n",
    "    \"_name_or_path\": \"ourModel\",\n",
    "    \"architectures\": [\n",
    "        \"BertForQuestionAnswering\"\n",
    "    ],\n",
    "    \"attention_probs_dropout_prob\": 0.1,\n",
    "    \"gradient_checkpointing\": False,\n",
    "    \"hidden_act\": \"gelu\",\n",
    "    \"hidden_dropout_prob\": 0.1,\n",
    "    \"hidden_size\": 1024,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"intermediate_size\": 4096,\n",
    "    \"layer_norm_eps\": 1e-12,\n",
    "    \"max_position_embeddings\": 512,\n",
    "    \"model_type\": \"bert\",\n",
    "    \"num_attention_heads\": 16,\n",
    "    \"num_hidden_layers\": 24,\n",
    "    \"pad_token_id\": 0,\n",
    "    \"position_embedding_type\": \"absolute\",\n",
    "    \"transformers_version\": \"4.17.0\",\n",
    "    \"type_vocab_size\": 2,\n",
    "    \"use_cache\": True,\n",
    "    \"vocab_size\": 30522\n",
    "})\n",
    "\n",
    "model = CustomBertForQuestionAnswering(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392\n",
      "392\n",
      "392\n",
      "392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get state dictionary of pre-trained model\n",
    "pretrained_dict = pretrained_model.state_dict()\n",
    "\n",
    "# Get state dictionary of custom model\n",
    "model_dict = model.state_dict()\n",
    "\n",
    "print(len(pretrained_dict))\n",
    "print(len(model_dict))\n",
    "\n",
    "\n",
    "# Check the keys that are not in the model_dict\n",
    "for k, v in pretrained_dict.items():\n",
    "    if k not in model_dict:\n",
    "        print(k, \":\", v.shape)\n",
    "\n",
    "# Filter out unnecessary keys\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "    \n",
    "\n",
    "print(len(pretrained_dict))\n",
    "print(len(model_dict))\n",
    "\n",
    "# Overwrite entries in the existing state dict\n",
    "model_dict.update(pretrained_dict)\n",
    "\n",
    "# Load the new state dict\n",
    "model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(model.training)\n",
    "model.eval()\n",
    "print(model.training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(pretrained_model.training)\n",
    "pretrained_model.eval()\n",
    "print(pretrained_model.training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'CustomBertForQuestionAnswering' is not supported for question-answering. Supported models are ['YosoForQuestionAnswering', 'NystromformerForQuestionAnswering', 'QDQBertForQuestionAnswering', 'FNetForQuestionAnswering', 'GPTJForQuestionAnswering', 'LayoutLMv2ForQuestionAnswering', 'RemBertForQuestionAnswering', 'CanineForQuestionAnswering', 'RoFormerForQuestionAnswering', 'BigBirdPegasusForQuestionAnswering', 'BigBirdForQuestionAnswering', 'ConvBertForQuestionAnswering', 'LEDForQuestionAnswering', 'DistilBertForQuestionAnswering', 'AlbertForQuestionAnswering', 'CamembertForQuestionAnswering', 'BartForQuestionAnswering', 'MBartForQuestionAnswering', 'LongformerForQuestionAnswering', 'XLMRobertaXLForQuestionAnswering', 'XLMRobertaForQuestionAnswering', 'RobertaForQuestionAnswering', 'SqueezeBertForQuestionAnswering', 'BertForQuestionAnswering', 'XLNetForQuestionAnsweringSimple', 'FlaubertForQuestionAnsweringSimple', 'MegatronBertForQuestionAnswering', 'MobileBertForQuestionAnswering', 'XLMForQuestionAnsweringSimple', 'ElectraForQuestionAnswering', 'ReformerForQuestionAnswering', 'FunnelForQuestionAnswering', 'LxmertForQuestionAnswering', 'MPNetForQuestionAnswering', 'DebertaForQuestionAnswering', 'DebertaV2ForQuestionAnswering', 'IBertForQuestionAnswering', 'SplinterForQuestionAnswering', 'Data2VecTextForQuestionAnswering'].\n"
     ]
    }
   ],
   "source": [
    "question_answerer = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 114/10570 [00:32<27:50,  6.26it/s] c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "  4%|▍         | 439/10570 [04:40<2:04:55,  1.35it/s]c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "  9%|▉         | 999/10570 [08:31<56:16,  2.83it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct:\t\t\t 942, out of 1000: 94.2%\n",
      "EM:\t\t\t 762, out of 1000: 76.2%\n",
      "BLEU:\t\t\t 878, out of 1000: 87.8%\n",
      "BLEU Score:\t\t\t 872.7723680467068, out of 1000: 87.27723680467068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 1999/10570 [13:49<51:12,  2.79it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct:\t\t\t 1869, out of 2000: 93.45%\n",
      "EM:\t\t\t 1432, out of 2000: 71.6%\n",
      "BLEU:\t\t\t 1703, out of 2000: 85.15%\n",
      "BLEU Score:\t\t\t 1697.4421047529827, out of 2000: 84.87210523764912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 2999/10570 [19:44<43:38,  2.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct:\t\t\t 2799, out of 3000: 93.3%\n",
      "EM:\t\t\t 2083, out of 3000: 69.43333333333334%\n",
      "BLEU:\t\t\t 2507, out of 3000: 83.56666666666666%\n",
      "BLEU Score:\t\t\t 2508.141037469119, out of 3000: 83.60470124897064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3999/10570 [26:03<46:35,  2.35it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct:\t\t\t 3728, out of 4000: 93.2%\n",
      "EM:\t\t\t 2750, out of 4000: 68.75%\n",
      "BLEU:\t\t\t 3308, out of 4000: 82.7%\n",
      "BLEU Score:\t\t\t 3317.115421181209, out of 4000: 82.92788552953022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 4999/10570 [41:24<2:08:46,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct:\t\t\t 4651, out of 5000: 93.02%\n",
      "EM:\t\t\t 3317, out of 5000: 66.34%\n",
      "BLEU:\t\t\t 4056, out of 5000: 81.12%\n",
      "BLEU Score:\t\t\t 4067.8638478020785, out of 5000: 81.35727695604156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 5999/10570 [53:28<53:14,  1.43it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct:\t\t\t 5601, out of 6000: 93.35%\n",
      "EM:\t\t\t 4042, out of 6000: 67.36666666666666%\n",
      "BLEU:\t\t\t 4891, out of 6000: 81.51666666666667%\n",
      "BLEU Score:\t\t\t 4909.089732703673, out of 6000: 81.81816221172788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 6999/10570 [1:07:14<27:44,  2.15it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct:\t\t\t 6494, out of 7000: 92.77142857142857%\n",
      "EM:\t\t\t 4693, out of 7000: 67.04285714285714%\n",
      "BLEU:\t\t\t 5667, out of 7000: 80.95714285714286%\n",
      "BLEU Score:\t\t\t 5691.8794216420865, out of 7000: 81.31256316631551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 7999/10570 [1:16:37<27:38,  1.55it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct:\t\t\t 7429, out of 8000: 92.8625%\n",
      "EM:\t\t\t 5390, out of 8000: 67.375%\n",
      "BLEU:\t\t\t 6491, out of 8000: 81.1375%\n",
      "BLEU Score:\t\t\t 6516.97302106118, out of 8000: 81.46216276326474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 8999/10570 [1:28:00<11:31,  2.27it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct:\t\t\t 8362, out of 9000: 92.91111111111111%\n",
      "EM:\t\t\t 5986, out of 9000: 66.5111111111111%\n",
      "BLEU:\t\t\t 7270, out of 9000: 80.77777777777777%\n",
      "BLEU Score:\t\t\t 7292.617317963844, out of 9000: 81.02908131070937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 9999/10570 [1:43:21<07:44,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct:\t\t\t 9279, out of 10000: 92.79%\n",
      "EM:\t\t\t 6578, out of 10000: 65.78%\n",
      "BLEU:\t\t\t 8042, out of 10000: 80.42%\n",
      "BLEU Score:\t\t\t 8068.880321520323, out of 10000: 80.68880321520324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10570/10570 [1:52:25<00:00,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 9789, out of 10570: 92.61116367076632%\n",
      "EM: 6901, out of 10570: 65.28855250709556%\n",
      "BLEU: 8477, out of 10570: 80.19867549668874%\n",
      "BLEU Score: 8501.452133048304, out of 10570: 80.43001071947307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "EM = 0\n",
    "BLEU = 0\n",
    "bblleeuu = 0\n",
    "errors = []\n",
    "for record in tqdm(validation):\n",
    "        # try:\n",
    "                total += 1\n",
    "                if (total % 1000 == 0):\n",
    "                        print(f\"Correct:\\t\\t {correct}, out of {total}: {100*correct/total}%\")\n",
    "                        print(f\"EM:\\t\\t\\t {EM}, out of {total}: {100*EM/total}%\")\n",
    "                        print(f\"BLEU:\\t\\t\\t {BLEU}, out of {total}: {100*BLEU/total}%\")\n",
    "                        print(f\"BLEU Score:\\t\\t {bblleeuu}, out of {total}: {100*bblleeuu/total}%\")\n",
    "                result = question_answerer(question=record['question'], context=record['context'], truncation=True, padding=True, return_tensors='pt')\n",
    "                # result = QA(model, tokenizer,record['question'], record['context'])\n",
    "                n = min(len(result['answer'].split()), 4)\n",
    "                # n = min(len(result.split()), 4)\n",
    "                if n == 0:\n",
    "                        BLEUscore = 0\n",
    "                else:\n",
    "                        weights = [1.0/n]*n\n",
    "                        BLEUscore = nltk.translate.bleu_score.sentence_bleu([record['answers']['text'][0].lower()], result['answer'].lower(), weights=weights)\n",
    "                        # BLEUscore = nltk.translate.bleu_score.sentence_bleu([record['answers']['text'][0].lower()], result.lower(), weights=weights)\n",
    "                if result['answer'] != '' and (result['answer'].lower() in record['answers']['text'][0].lower() or record['answers']['text'][0].lower() in result['answer'].lower()):\n",
    "                # if result != '' and (result.lower() in record['answers']['text'][0].lower() or record['answers']['text'][0].lower() in result.lower()):\n",
    "                        correct += 1\n",
    "                if record['answers']['text'][0].lower() == result['answer'].lower():\n",
    "                # if record['answers']['text'][0].lower() == result.lower():\n",
    "                        EM += 1\n",
    "                if BLEUscore > 0.5:\n",
    "                        BLEU += 1\n",
    "                bblleeuu += BLEUscore\n",
    "                 \n",
    "        # except Exception as e:\n",
    "        #         errors.append(total)\n",
    "        #         print(f\"Error at {total}: {e}\")\n",
    "        #         continue\n",
    "print(f\"Correct: {correct}, out of {total}: {100*correct/total}%\")\n",
    "print(f\"EM: {EM}, out of {total}: {100*EM/total}%\")\n",
    "print(f\"BLEU: {BLEU}, out of {total}: {100*BLEU/total}%\")\n",
    "print(f\"BLEU Score: {bblleeuu}, out of {total}: {100*bblleeuu/total}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dh bl pipeline el ndeefa, el tany aw7ash men dyh\n",
    "# 1%|          | 114/10570 [00:32<27:50,  6.26it/s] c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
    "# The hypothesis contains 0 counts of 3-gram overlaps.\n",
    "# Therefore the BLEU score evaluates to 0, independently of\n",
    "# how many N-gram overlaps of lower order it contains.\n",
    "# Consider using lower n-gram order or use SmoothingFunction()\n",
    "#   warnings.warn(_msg)\n",
    "# c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
    "# The hypothesis contains 0 counts of 4-gram overlaps.\n",
    "# Therefore the BLEU score evaluates to 0, independently of\n",
    "# how many N-gram overlaps of lower order it contains.\n",
    "# Consider using lower n-gram order or use SmoothingFunction()\n",
    "#   warnings.warn(_msg)\n",
    "#   4%|▍         | 439/10570 [04:40<2:04:55,  1.35it/s]c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
    "# The hypothesis contains 0 counts of 2-gram overlaps.\n",
    "# Therefore the BLEU score evaluates to 0, independently of\n",
    "# how many N-gram overlaps of lower order it contains.\n",
    "# Consider using lower n-gram order or use SmoothingFunction()\n",
    "#   warnings.warn(_msg)\n",
    "#   9%|▉         | 999/10570 [08:31<56:16,  2.83it/s]  \n",
    "# Correct:\t\t\t 942, out of 1000: 94.2%\n",
    "# EM:\t\t\t 762, out of 1000: 76.2%\n",
    "# BLEU:\t\t\t 878, out of 1000: 87.8%\n",
    "# BLEU Score:\t\t\t 872.7723680467068, out of 1000: 87.27723680467068\n",
    "#  19%|█▉        | 1999/10570 [13:49<51:12,  2.79it/s]  \n",
    "# Correct:\t\t\t 1869, out of 2000: 93.45%\n",
    "# EM:\t\t\t 1432, out of 2000: 71.6%\n",
    "# BLEU:\t\t\t 1703, out of 2000: 85.15%\n",
    "# BLEU Score:\t\t\t 1697.4421047529827, out of 2000: 84.87210523764912\n",
    "#  28%|██▊       | 2999/10570 [19:44<43:38,  2.89it/s]  \n",
    "# Correct:\t\t\t 2799, out of 3000: 93.3%\n",
    "# EM:\t\t\t 2083, out of 3000: 69.43333333333334%\n",
    "# BLEU:\t\t\t 2507, out of 3000: 83.56666666666666%\n",
    "# BLEU Score:\t\t\t 2508.141037469119, out of 3000: 83.60470124897064\n",
    "#  38%|███▊      | 3999/10570 [26:03<46:35,  2.35it/s]  \n",
    "# Correct:\t\t\t 3728, out of 4000: 93.2%\n",
    "# EM:\t\t\t 2750, out of 4000: 68.75%\n",
    "# BLEU:\t\t\t 3308, out of 4000: 82.7%\n",
    "# BLEU Score:\t\t\t 3317.115421181209, out of 4000: 82.92788552953022\n",
    "#  47%|████▋     | 4999/10570 [41:24<2:08:46,  1.39s/it]\n",
    "# Correct:\t\t\t 4651, out of 5000: 93.02%\n",
    "# EM:\t\t\t 3317, out of 5000: 66.34%\n",
    "# BLEU:\t\t\t 4056, out of 5000: 81.12%\n",
    "# BLEU Score:\t\t\t 4067.8638478020785, out of 5000: 81.35727695604156\n",
    "#  57%|█████▋    | 5999/10570 [53:28<53:14,  1.43it/s]  \n",
    "# Correct:\t\t\t 5601, out of 6000: 93.35%\n",
    "# EM:\t\t\t 4042, out of 6000: 67.36666666666666%\n",
    "# BLEU:\t\t\t 4891, out of 6000: 81.51666666666667%\n",
    "# BLEU Score:\t\t\t 4909.089732703673, out of 6000: 81.81816221172788\n",
    "#  66%|██████▌   | 6999/10570 [1:07:14<27:44,  2.15it/s]  \n",
    "# Correct:\t\t\t 6494, out of 7000: 92.77142857142857%\n",
    "# EM:\t\t\t 4693, out of 7000: 67.04285714285714%\n",
    "# BLEU:\t\t\t 5667, out of 7000: 80.95714285714286%\n",
    "# BLEU Score:\t\t\t 5691.8794216420865, out of 7000: 81.31256316631551\n",
    "#  76%|███████▌  | 7999/10570 [1:16:37<27:38,  1.55it/s]  \n",
    "# Correct:\t\t\t 7429, out of 8000: 92.8625%\n",
    "# EM:\t\t\t 5390, out of 8000: 67.375%\n",
    "# BLEU:\t\t\t 6491, out of 8000: 81.1375%\n",
    "# BLEU Score:\t\t\t 6516.97302106118, out of 8000: 81.46216276326474\n",
    "#  85%|████████▌ | 8999/10570 [1:28:00<11:31,  2.27it/s]  \n",
    "# Correct:\t\t\t 8362, out of 9000: 92.91111111111111%\n",
    "# EM:\t\t\t 5986, out of 9000: 66.5111111111111%\n",
    "# BLEU:\t\t\t 7270, out of 9000: 80.77777777777777%\n",
    "# BLEU Score:\t\t\t 7292.617317963844, out of 9000: 81.02908131070937\n",
    "#  95%|█████████▍| 9999/10570 [1:43:21<07:44,  1.23it/s]\n",
    "# Correct:\t\t\t 9279, out of 10000: 92.79%\n",
    "# EM:\t\t\t 6578, out of 10000: 65.78%\n",
    "# BLEU:\t\t\t 8042, out of 10000: 80.42%\n",
    "# BLEU Score:\t\t\t 8068.880321520323, out of 10000: 80.68880321520324\n",
    "# 100%|██████████| 10570/10570 [1:52:25<00:00,  1.57it/s]\n",
    "# Correct: 9789, out of 10570: 92.61116367076632%\n",
    "# EM: 6901, out of 10570: 65.28855250709556%\n",
    "# BLEU: 8477, out of 10570: 80.19867549668874%\n",
    "# BLEU Score: 8501.452133048304, out of 10570: 80.43001071947307"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
