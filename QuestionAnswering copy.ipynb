{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/DELL/.cache/huggingface/datasets/parquet/plain_text-57edf78d6033ac9a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "920550cee73043b0be0eac223083c2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import transformers\n",
    "from datasets import load_dataset\n",
    "# from transformers import AutoModelForQuestionAnswering, BertModel, BertConfig, BertTokenizer, pipeline, AutoTokenizer\n",
    "from transformers import AutoModelForQuestionAnswering, BertConfig, BertTokenizer, pipeline, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import nltk\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"squad\")\n",
    "train = dataset['train']\n",
    "validation = dataset['validation']\n",
    "\n",
    "# question_answerer = pipeline(\"question-answering\", model='AliHashish/distilbert-base-uncased-finetuned-squad-EZcufe')\n",
    "model_checkpoint = \"atharvamundada99/bert-large-question-answering-finetuned-legal\"\n",
    "pretrained_model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QA(model, tokenizer, question, context):\n",
    "    # Process the inputs\n",
    "    inputs = tokenizer(question, context, return_tensors='pt')\n",
    "\n",
    "    # Pass the inputs through the model and get the start and end scores\n",
    "    start_scores, end_scores = model(**inputs)\n",
    "\n",
    "    # Get the start and end positions\n",
    "    start_position = torch.argmax(start_scores)\n",
    "    end_position = torch.argmax(end_scores)\n",
    "\n",
    "    # Get the answer\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][start_position:end_position+1]))\n",
    "\n",
    "    return answer\n",
    "\n",
    "def QAs(model, tokenizer, questions, contexts):\n",
    "    answers = []\n",
    "    for question, context in zip(questions, contexts):\n",
    "        answer = QA(model, tokenizer, question, context)\n",
    "        answers.append(answer)\n",
    "    return answers\n",
    "\n",
    "def Evaluation(model, tokenizer, validation):\n",
    "    correct = 0\n",
    "    EM = 0\n",
    "    total = 0\n",
    "    errors = []\n",
    "    for record in tqdm(validation):\n",
    "        try:\n",
    "            total += 1\n",
    "            if (total % 500 == 0):\n",
    "                print(f\"\\nAccuracy: {100*correct/total}\")\n",
    "                print(f\"Correct: {correct}, out of {total}\")\n",
    "                print(f\"EM: {100*EM/total}\")\n",
    "                print(f\"EM Correct: {EM}, out of {total}\\n\")\n",
    "\n",
    "            predicted_answer = QA(model, tokenizer, record['question'], record['context'])\n",
    "            if predicted_answer.lower() in record['answers']['text'][0].lower() or record['answers']['text'][0].lower() in predicted_answer.lower():\n",
    "                correct += 1\n",
    "            if predicted_answer.lower() == record['answers']['text'][0].lower():\n",
    "                EM += 1\n",
    "        except Exception as e:\n",
    "            errors.append(total)\n",
    "            print(f\"Error at {total}: {e} \")\n",
    "            continue\n",
    "    return correct, EM, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BertConfig:\n",
    "#     def __init__(self, vocab_size=30522, hidden_size=1024, num_hidden_layers=24, intermediate_size=4096, num_attention_heads=16, attention_probs_dropout_prob=0.1, max_position_embeddings=512, type_vocab_size=2, **kwargs):\n",
    "#         self.vocab_size = vocab_size\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_hidden_layers = num_hidden_layers\n",
    "#         self.intermediate_size = intermediate_size\n",
    "#         self.num_attention_heads = num_attention_heads\n",
    "#         self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "#         self.max_position_embeddings = max_position_embeddings\n",
    "#         self.type_vocab_size = type_vocab_size\n",
    "#         for key, value in kwargs.items():\n",
    "#             setattr(self, key, value)\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_dict(cls, json_object):\n",
    "#         return cls(**json_object)\n",
    "\n",
    "#     def to_dict(self):\n",
    "#         return self.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# class BertEmbeddings(nn.Module):\n",
    "#     def __init__(self, vocab_size=30522, hidden_size=1024, pad_token_id=0, max_position_embeddings=512, type_vocab_size=2):\n",
    "#         super(BertEmbeddings, self).__init__()\n",
    "#         self.word_embeddings = nn.Embedding(vocab_size, hidden_size, padding_idx=pad_token_id)\n",
    "#         self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_size)\n",
    "#         self.token_type_embeddings = nn.Embedding(type_vocab_size, hidden_size)\n",
    "#         self.position_ids = torch.arange(max_position_embeddings).unsqueeze(0)\n",
    "\n",
    "#         # LayerNorm and dropout Module\n",
    "#         self.LayerNorm = nn.LayerNorm(hidden_size, eps=1e-12)\n",
    "#         self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "#     def forward(self, input_ids=None, token_type_ids=None, position_ids=None):\n",
    "#         seq_length = input_ids.size(1)\n",
    "#         position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)  # create position_ids on the fly\n",
    "#         position_embeddings = self.position_embeddings(position_ids)\n",
    "\n",
    "#         if token_type_ids is None:\n",
    "#             token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "#         word_embeddings = self.word_embeddings(input_ids)\n",
    "#         token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "#         embeddings = word_embeddings + position_embeddings + token_type_embeddings\n",
    "#         embeddings = self.LayerNorm(embeddings)\n",
    "#         embeddings = self.dropout(embeddings)\n",
    "#         return embeddings\n",
    "\n",
    "class BertEmbeddings(nn.Module):\n",
    "    def __init__(self, vocab_size=30522, hidden_size=1024, pad_token_id=0, max_position_embeddings=512, type_vocab_size=2):\n",
    "        super(BertEmbeddings, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, hidden_size, padding_idx=pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(type_vocab_size, hidden_size)\n",
    "\n",
    "        # Make position_ids a nn.Parameter\n",
    "        self.position_ids = nn.Parameter(torch.arange(max_position_embeddings).unsqueeze(0), requires_grad=False)\n",
    "\n",
    "        # LayerNorm and dropout Module\n",
    "        self.LayerNorm = nn.LayerNorm(hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, input_ids=None, token_type_ids=None, position_ids=None):\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, :input_ids.size(1)]  # use pre-computed position_ids\n",
    "\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        word_embeddings = self.word_embeddings(input_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = word_embeddings + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfAttention(nn.Module):\n",
    "    # def __init__(self, hidden_size=1024, num_attention_heads=16, attention_probs_dropout_prob=0.1):\n",
    "    #     super(BertSelfAttention, self).__init__()\n",
    "    #     self.query = nn.Linear(hidden_size, hidden_size)\n",
    "    #     self.key = nn.Linear(hidden_size, hidden_size)\n",
    "    #     self.value = nn.Linear(hidden_size, hidden_size)\n",
    "    #     self.dropout = nn.Dropout(attention_probs_dropout_prob)\n",
    "\n",
    "    # def forward(self, hidden_states, attention_mask=None):\n",
    "    #     # Implement the forward pass\n",
    "    #     query_states = self.query(hidden_states)\n",
    "    #     key_states = self.key(hidden_states)\n",
    "    #     value_states = self.value(hidden_states)\n",
    "\n",
    "    #     # Compute the dot product between query and key states\n",
    "    #     attention_scores = torch.matmul(query_states, key_states.transpose(-1, -2))\n",
    "\n",
    "    #     # Apply the attention mask, if provided\n",
    "    #     if attention_mask is not None:\n",
    "    #         attention_scores = attention_scores + attention_mask\n",
    "\n",
    "    #     # Apply softmax to get the attention probabilities\n",
    "    #     attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "\n",
    "    #     # Apply dropout\n",
    "    #     attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "    #     # Multiply the attention probabilities with the value states\n",
    "    #     context_layer = torch.matmul(attention_probs, value_states)\n",
    "\n",
    "    #     return context_layer\n",
    "    #     # pass\n",
    "    \n",
    "    def __init__(self, hidden_size, num_attention_heads, dropout_prob):\n",
    "        super(BertSelfAttention, self).__init__()\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.attention_head_size = int(hidden_size / num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        if attention_mask is not None:\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        return context_layer\n",
    "\n",
    "class BertSelfOutput(nn.Module):\n",
    "    def __init__(self, hidden_size=1024, dropout_prob=0.1):\n",
    "        super(BertSelfOutput, self).__init__()\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        # Implement the forward pass\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "        # pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"atharvamundada99/bert-large-question-answering-finetuned-legal\"\n",
    "# pretrained_model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, hidden_size=1024, num_attention_heads=16, attention_probs_dropout_prob=0.1):\n",
    "        super(BertAttention, self).__init__()\n",
    "\n",
    "        self.self = BertSelfAttention(hidden_size, num_attention_heads, attention_probs_dropout_prob)       # ~dh el bayez~ sala7to 5las\n",
    "        self.output = BertSelfOutput(hidden_size, attention_probs_dropout_prob)     # tmam\n",
    "\n",
    "        # pretrained_model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "        # self.self = pretrained_model.bert.encoder.layer[0].attention.self\n",
    "        # # self.output = pretrained_model.bert.encoder.layer[0].attention.output\n",
    "        # del pretrained_model\n",
    "\n",
    "    def forward(self, input_tensor, attention_mask):        # tmam\n",
    "        # Implement the forward pass\n",
    "        self_output = self.self(input_tensor, attention_mask)\n",
    "        if isinstance(self_output, tuple):\n",
    "            self_output = self_output[0]\n",
    "        attention_output = self.output(self_output, input_tensor)\n",
    "        return attention_output\n",
    "        # pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GELUActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.gelu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self, hidden_size=1024, intermediate_size=4096):\n",
    "        super(BertIntermediate, self).__init__()\n",
    "        self.dense = nn.Linear(hidden_size, intermediate_size)\n",
    "        # self.intermediate_act_fn = F.gelu\n",
    "        self.intermediate_act_fn = GELUActivation()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        # hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "        hidden_states = F.gelu(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BertOutput(nn.Module):\n",
    "    def __init__(self, intermediate_size=4096, hidden_size=1024, dropout_prob=0.1):\n",
    "        super(BertOutput, self).__init__()\n",
    "        self.dense = nn.Linear(intermediate_size, hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        # Implement the forward pass\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "        # pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# model_checkpoint = \"atharvamundada99/bert-large-question-answering-finetuned-legal\"\n",
    "# pretrained_model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, hidden_size=1024, intermediate_size=4096, num_attention_heads=16, attention_probs_dropout_prob=0.1):\n",
    "        super(BertLayer, self).__init__()\n",
    "        self.attention = BertAttention(hidden_size, num_attention_heads, attention_probs_dropout_prob)      # bayez\n",
    "        self.intermediate = BertIntermediate(hidden_size, intermediate_size)                        # tmam\n",
    "        self.output = BertOutput(intermediate_size, hidden_size, attention_probs_dropout_prob)      # tmam\n",
    "\n",
    "        # pretrained_model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "        # bert_layer = pretrained_model.bert.encoder.layer[0] \n",
    "        # self.attention = bert_layer.attention\n",
    "        # self.intermediate = bert_layer.intermediate\n",
    "        # self.output = bert_layer.output\n",
    "        # del pretrained_model\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):           # tmam\n",
    "        # Implement the forward pass\n",
    "        attention_output = self.attention(hidden_states, attention_mask)\n",
    "        if isinstance(attention_output, tuple):\n",
    "                attention_output = attention_output[0]\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output\n",
    "        # pass\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, num_hidden_layers=24, hidden_size=1024, intermediate_size=4096, num_attention_heads=16, attention_probs_dropout_prob=0.1):\n",
    "        super(BertEncoder, self).__init__()\n",
    "        self.layer = nn.ModuleList([BertLayer(hidden_size, intermediate_size, num_attention_heads, attention_probs_dropout_prob) for _ in range(num_hidden_layers)])\n",
    "        \n",
    "        # pretrained_model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "        # self.layer = nn.ModuleList(pretrained_model.bert.encoder.layer)\n",
    "\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):       # 8albn tmam, b3d 7etet el isinstance dyh\n",
    "        # Implement the forward pass\n",
    "        for layer in self.layer:\n",
    "            # check type of hidden_states\n",
    "            if isinstance(hidden_states, tuple):\n",
    "                hidden_states = hidden_states[0]\n",
    "            hidden_states = layer(hidden_states, attention_mask)\n",
    "        return hidden_states\n",
    "        # pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "# class BertPooler(nn.Module):\n",
    "#     def __init__(self, hidden_size=1024):\n",
    "#         super(BertPooler, self).__init__()\n",
    "#         self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.activation = nn.Tanh()\n",
    "\n",
    "#     def forward(self, hidden_states):\n",
    "#         # We \"pool\" the model by simply taking the hidden state corresponding to the first token.\n",
    "#         first_token_tensor = hidden_states[:, 0]\n",
    "#         pooled_output = self.dense(first_token_tensor)\n",
    "#         pooled_output = self.activation(pooled_output)\n",
    "#         return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# import transformers\n",
    "\n",
    "# model_checkpoint = \"atharvamundada99/bert-large-question-answering-finetuned-legal\"\n",
    "# pretrained_model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "\n",
    "class BertModel(nn.Module):\n",
    "    def __init__(self, vocab_size=30522, hidden_size=1024, num_hidden_layers=24, intermediate_size=4096, num_attention_heads=16, attention_probs_dropout_prob=0.1, pad_token_id = 0, max_position_embeddings=512, type_vocab_size=2):\n",
    "        super(BertModel, self).__init__()\n",
    "        # hgrb b2a ast5dm el built in embeddings w encoder, w ashoof accuracy, lw tmam yb2a el forward el hena dyh tmam [DONE]\n",
    "        self.embeddings = BertEmbeddings(vocab_size, hidden_size, pad_token_id, max_position_embeddings, type_vocab_size)       # el embeddings kolaha tmam\n",
    "        # self.embeddings = pretrained_model.embeddings\n",
    "        # self.encoder = pretrained_model.encoder\n",
    "        self.encoder = BertEncoder(num_hidden_layers, hidden_size, intermediate_size, num_attention_heads, attention_probs_dropout_prob)      # el moshkela fy dh\n",
    "        \n",
    "\n",
    "        # self.pooler = BertPooler(hidden_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):     # tmam\n",
    "        # Implement the forward pass\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
    "        encoder_output = self.encoder(embedding_output, extended_attention_mask)\n",
    "        # pooled_output = self.pooler(encoder_output)\n",
    "\n",
    "        return encoder_output\n",
    "        # return pooled_output  # or return pooled_output lw hnst3ml el pooler\n",
    "        # pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# import transformers\n",
    "# model_checkpoint = \"atharvamundada99/bert-large-question-answering-finetuned-legal\"\n",
    "\n",
    "class CustomBertForQuestionAnswering(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(CustomBertForQuestionAnswering, self).__init__()\n",
    "        self.config = config\n",
    "        self.bert = BertModel(vocab_size=config.vocab_size, hidden_size=config.hidden_size, num_hidden_layers=config.num_hidden_layers, intermediate_size=config.intermediate_size, num_attention_heads=config.num_attention_heads, attention_probs_dropout_prob=config.attention_probs_dropout_prob, pad_token_id=config.pad_token_id ,max_position_embeddings=config.max_position_embeddings, type_vocab_size=config.type_vocab_size)\n",
    "        # self.bert = transformers.BertModel.from_pretrained(model_checkpoint)\n",
    "        \n",
    "        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):     # tmam\n",
    "        # outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        # sequence_output = outputs.last_hidden_state\n",
    "        sequence_output = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "        if isinstance(sequence_output, tuple):\n",
    "            sequence_output = sequence_output[0]\n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "        return start_logits, end_logits\n",
    "\n",
    "# Instantiate the model with the provided configuration\n",
    "config = BertConfig.from_dict({\n",
    "    \"_name_or_path\": \"ourModel\",\n",
    "    \"architectures\": [\n",
    "        \"BertForQuestionAnswering\"\n",
    "    ],\n",
    "    \"attention_probs_dropout_prob\": 0.1,\n",
    "    \"gradient_checkpointing\": False,\n",
    "    \"hidden_act\": \"gelu\",\n",
    "    \"hidden_dropout_prob\": 0.1,\n",
    "    \"hidden_size\": 1024,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"intermediate_size\": 4096,\n",
    "    \"layer_norm_eps\": 1e-12,\n",
    "    \"max_position_embeddings\": 512,\n",
    "    \"model_type\": \"bert\",\n",
    "    \"num_attention_heads\": 16,\n",
    "    \"num_hidden_layers\": 24,\n",
    "    \"pad_token_id\": 0,\n",
    "    \"position_embedding_type\": \"absolute\",\n",
    "    \"transformers_version\": \"4.17.0\",\n",
    "    \"type_vocab_size\": 2,\n",
    "    \"use_cache\": True,\n",
    "    \"vocab_size\": 30522\n",
    "})\n",
    "\n",
    "model = CustomBertForQuestionAnswering(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392\n",
      "392\n",
      "392\n",
      "392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get state dictionary of pre-trained model\n",
    "pretrained_dict = pretrained_model.state_dict()\n",
    "\n",
    "# Get state dictionary of custom model\n",
    "model_dict = model.state_dict()\n",
    "\n",
    "print(len(pretrained_dict))\n",
    "print(len(model_dict))\n",
    "\n",
    "\n",
    "# Check the keys that are not in the model_dict\n",
    "for k, v in pretrained_dict.items():\n",
    "    if k not in model_dict:\n",
    "        print(k, \":\", v.shape)\n",
    "\n",
    "# Filter out unnecessary keys\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "\n",
    "\n",
    "# removed_keys = []\n",
    "# for k, v in pretrained_dict.items():\n",
    "#     if k in model_dict:\n",
    "#         model_dict[k] = v\n",
    "#     elif \"bert.\"+k in model_dict:\n",
    "#         model_dict[\"bert.\"+k] = v\n",
    "#     else:\n",
    "#         # remove this key\n",
    "#         removed_keys.append(k)\n",
    "# print(f\"Removed {len(removed_keys)} keys\")\n",
    "# for k in removed_keys:\n",
    "#     del pretrained_dict[k]\n",
    "    \n",
    "\n",
    "print(len(pretrained_dict))\n",
    "print(len(model_dict))\n",
    "\n",
    "# Overwrite entries in the existing state dict\n",
    "model_dict.update(pretrained_dict)\n",
    "\n",
    "# Load the new state dict\n",
    "model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(model.training)\n",
    "model.eval()\n",
    "print(model.training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(pretrained_model.training)\n",
    "pretrained_model.eval()\n",
    "print(pretrained_model.training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 19/10570 [00:07<1:13:27,  2.39it/s]c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "  0%|          | 42/10570 [00:16<1:06:40,  2.63it/s]c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "  2%|▏         | 199/10570 [01:03<54:58,  3.14it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct:\t 155, out of 200: 77.5%\n",
      "EM:\t\t\t 137, out of 200: 68.5%\n",
      "BLEU:\t\t\t 173, out of 200: 86.5%\n",
      "Correct: 155, out of 200: 77.5%\n",
      "EM: 137, out of 200: 68.5%\n",
      "BLEU: 173, out of 200: 86.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "EM = 0\n",
    "BLEU = 0\n",
    "errors = []\n",
    "for record in tqdm(validation):\n",
    "        # try:\n",
    "                total += 1\n",
    "                if total == 1001:\n",
    "                        break\n",
    "                if (total % 200 == 0):\n",
    "                        print(f\"Correct:\\t {correct}, out of {total}: {100*correct/total}%\")\n",
    "                        print(f\"EM:\\t\\t\\t {EM}, out of {total}: {100*EM/total}%\")\n",
    "                        print(f\"BLEU:\\t\\t\\t {BLEU}, out of {total}: {100*BLEU/total}%\")\n",
    "                        break\n",
    "                # result = question_answerer(question=record['question'], context=record['context'], truncation=True, padding=True, return_tensors='pt')\n",
    "                result = QA(model, tokenizer,record['question'], record['context'])\n",
    "                # n = min(len(result['answer'].split()), 4)\n",
    "                n = min(len(result.split()), 4)\n",
    "                if n == 0:\n",
    "                        BLEUscore = 0\n",
    "                else:\n",
    "                        weights = [1.0/n]*n\n",
    "                        BLEUscore = nltk.translate.bleu_score.sentence_bleu([record['answers']['text'][0].lower()], result.lower(), weights=weights)\n",
    "                if result != '' and (result.lower() in record['answers']['text'][0].lower() or record['answers']['text'][0].lower() in result.lower()):\n",
    "                        correct += 1\n",
    "                if record['answers']['text'][0].lower() == result.lower():\n",
    "                        EM += 1\n",
    "                if BLEUscore > 0.5:\n",
    "                        BLEU += 1\n",
    "        # except Exception as e:\n",
    "        #         errors.append(total)\n",
    "        #         print(f\"Error at {total}: {e}\")\n",
    "        #         continue\n",
    "print(f\"Correct: {correct}, out of {total}: {100*correct/total}%\")\n",
    "print(f\"EM: {EM}, out of {total}: {100*EM/total}%\")\n",
    "print(f\"BLEU: {BLEU}, out of {total}: {100*BLEU/total}%\")\n",
    "# bl training on:\n",
    "# Correct: 152, out of 200: 76.0%\n",
    "# EM: 136, out of 200: 68.0%\n",
    "# BLEU: 173, out of 200: 86.5%\n",
    "\n",
    "# bl eval b3d ma 3ml training:\n",
    "# Correct: 155, out of 200: 77.5%\n",
    "# EM: 137, out of 200: 68.5%\n",
    "# BLEU: 173, out of 200: 86.5%\n",
    "\n",
    "# bl eval mn el awl:\n",
    "# Correct: 155, out of 200: 77.5%\n",
    "# EM: 137, out of 200: 68.5%\n",
    "# BLEU: 173, out of 200: 86.5%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
